# -*- coding: utf-8 -*-
"""webscrap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KYVH7FiTt98Nrg2Dgr4F495UpCbKPG9w

# Importation des librarry
"""

import pandas as pd
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, SnowballStemmer
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import plotly.express as px
from dash import Dash, dcc, html
from dash.dependencies import Input, Output
import base64
import nest_asyncio
import plotly.express as px
from dash import Dash, dcc, html
import dash_bootstrap_components as dbc
import base64
import nest_asyncio
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import io

"""# Scaping web

**site de ansm**
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# DataFrame pour stocker les résultats
df = pd.DataFrame(columns=['Title', 'Author', 'Date', 'Number of Responses', 'Content'])

def scrape_post_content(post_url):
    response = requests.get(post_url)
    soup = BeautifulSoup(response.content, 'html.parser')
    post_content_tag = soup.find('div', class_='bbWrapper')
    post_content = post_content_tag.text.strip() if post_content_tag else "No content found"
    return post_content

def scrape_forum_page(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Les threads
    threads = soup.find_all('div', class_='structItem--thread')

    for thread in threads:
        title_tag = thread.find('div', class_='structItem-title')
        title = title_tag.text.strip()
        author = thread.find('a', class_='username').text.strip()
        date = thread.find('time')['datetime']

        # Nombre de réponses
        response_count_tag = thread.find('dl', class_='pairs pairs--justified structItem-minor')
        if response_count_tag:
            response_counts = response_count_tag.find_all('dd')
            if len(response_counts) > 1:
                num_responses = response_counts[1].text.strip()
            else:
                num_responses = '0'
        else:
            num_responses = '0'

        # Ontention complet du post
        post_url = 'https://forum.alzheimers.org.uk' + title_tag.find('a')['href']
        content = scrape_post_content(post_url)

        # Les données au DataFrame
        df.loc[len(df)] = [title, author, date, num_responses, content]

# Les 30 premières pages (600 postes)
base_url = 'https://forum.alzheimers.org.uk/forums/i-have-dementia.56/page-{}.html'
for page_num in range(1, 31):
    url = base_url.format(page_num)
    print(f'Scraping page {page_num}...')
    scrape_forum_page(url)
    time.sleep(1)

# Fichier CSV
df.to_csv('forum_posts_30_pages.csv', index=False)

print(df.head())

"""**Forum alzheimer's Society**"""

import requests
from bs4 import BeautifulSoup
import json
import pandas as pd
import urllib3

urllib3.disable_warnings()

url_list = [
    'https://data.ansm.sante.fr/specialite/62650485',
    'https://data.ansm.sante.fr/specialite/68673252',
    'https://data.ansm.sante.fr/specialite/66072114'
]

All_data_med = []
x = 0

# DataFrames pour stocker les données
df_medications = pd.DataFrame(columns=[
    'Nom_medicament', 'Estimation',
    'Hommes (%)',
    'Femmes (%)',
    '20-59 (%)',
    '60+ (%)',
    'Période_min', 'Période_max',
    'Total_declarations',
    'Hommes_decl (%)',
    'Femmes_decl (%)'
])
df_effects = pd.DataFrame(columns=[
    'Nom_medicament', 'Pathologie', 'Valeur'
])

for url in url_list:
    html_content = requests.get(url, verify=False).text
    soup = BeautifulSoup(html_content, "lxml")

    # Extraction des informations de base
    Nom_medicament = soup.find('div', attrs={'class': 'text-xl lg:text-2xl font-medium mb-4'})
    Estimation = soup.find('h3', attrs={'class': 'text-2xl text-primary'})

    if Nom_medicament:
        Nom_medicament = Nom_medicament.string
    else:
        Nom_medicament = "Unknown"

    if Estimation:
        Estimation = Estimation.string
    else:
        Estimation = "Unknown"

    script = soup.find('script', attrs={'id': "__NEXT_DATA__"}).string
    script2 = json.loads(script)

    Nom_medicament = script2["props"]["pageProps"]["cis"]['name']

    pat_trait_hom = script2["props"]["pageProps"]["cis"]["substances"][0]["repartitionPerGender"]["male"]
    pat_trait_fem = script2["props"]["pageProps"]["cis"]["substances"][0]["repartitionPerGender"]["female"]

    age_data = script2["props"]["pageProps"]["cis"]["substances"][0]['repartitionPerAge']
    age_20_59 = age_data[0] if len(age_data) > 0 else {'range': '20-59', 'valuePercent': None}
    age_60_p = age_data[1] if len(age_data) > 1 else {'range': '60+', 'valuePercent': None}

    periode_effects = script2["props"]["pageProps"]["cis"]["substances"][0]['sideEffects']["bnpvPeriod"]
    total_declarations = script2["props"]["pageProps"]["cis"]["substances"][0]['sideEffects']["declarations"]["total"]

    decl_hom = script2["props"]["pageProps"]["cis"]["substances"][0]['sideEffects']["repartitionPerGender"]["male"]
    decl_fem = script2["props"]["pageProps"]["cis"]["substances"][0]['sideEffects']["repartitionPerGender"]["female"]

    # Les données démographiques au DataFrame
    new_row = {
        'Nom_medicament': Nom_medicament,
        'Estimation': Estimation,
        'Hommes (%)': pat_trait_hom['valuePercent'],
        'Femmes (%)': pat_trait_fem['valuePercent'],
        '20-59 (%)': age_20_59['valuePercent'],
        '60+ (%)': age_60_p['valuePercent'],
        'Période_min': periode_effects['minYear'],
        'Période_max': periode_effects['maxYear'],
        'Total_declarations': total_declarations,
        'Hommes_decl (%)': decl_hom['valuePercent'],
        'Femmes_decl (%)': decl_fem['valuePercent']
    }

    df_medications = pd.concat([df_medications, pd.DataFrame([new_row])], ignore_index=True)

    # Les effets indésirables au DataFrame
    side_effects = script2["props"]["pageProps"]["cis"]["substances"][0]['sideEffects']['repartitionPerPathology']
    for effect in side_effects:
        new_effect = {
            'Nom_medicament': Nom_medicament,
            'Pathologie': effect['range'],
            'Valeur': effect['value']
        }
        df_effects = pd.concat([df_effects, pd.DataFrame([new_effect])], ignore_index=True)

# Fichiers CSV
df_medications.to_csv('medications.csv', index=False)
df_effects.to_csv('effects.csv', index=False)

# Vérification
print(df_medications.head())
print(df_effects.head())

